[{"content":"Hi, I\u0026rsquo;m Hans Ollivander (han5 011ivand3r). I am a student and dreamer. As a student, I ask questions and seek for digestible answers. Generally, it is followed by me trying to contradict my brain with wrong answers. And a dreamer, because that\u0026rsquo;s where freedom is. I am a loyal human(i.e. humanity over everything).\nI generally divide my time between schoolwork, coding and making YouTube videos. Right now I\u0026rsquo;m enrolled in a Dual Degree course (B.Tech + M.Tech) in Mechanical Engineering at IIT Kharagpur as Shubham Hansda(not han5 011ivand3r). Sleep, Eating, Workout, Chores\u0026hellip; always take up some time.\nI am passionate about human space exploration, machine learning and storytelling. I often find myself dabbling in the reality of fickleness of carbon-based life on planet Earth. I consume books, articles, podcasts and videos on them. And talk to people about them (Not as much I want to! So why not get in touch!).\nI also make YouTube videos on (football/soccer + data science). Visit my YouTube Channel for more.\n \nEnough about me. tTales is all about dumping my thoughts and opinions into one place on the internet.\ntTales is created with HUGO. The theme I\u0026rsquo;m using is minimo. I customized it as per my needs, and it\u0026rsquo;s currently hosted on GitHub. The reason I chose minimo over other themes is the search bar and obviously the UI üòç.\nCheck out the HUGO site. They have hundreds of exciting themes, and it\u0026rsquo;s built on Go. So, it\u0026rsquo;s faster than other static site generators and easy to set up. Now that I‚Äôve justified my choices, thanks for visiting my website!\nGet in touch using any of the links in the footer!\n","href":"/tTales/about/","title":"About"},{"content":"Whenever one starts learning about Machine Learning algorithms the first thing one learns is Linear Regression. But the way people generally blaze through it because it\u0026rsquo;s rudimentary and label it as very intuitive doesn\u0026rsquo;t sit well with me. So, in this article we will build a Linear Regression model with the help of probability.\nThe main idea in a regression problem is to determine the strength of relation between a bunch of independent variables \\(x = (x_1, x_2, \u0026hellip; x_n)\\) and a dependent variable \\(y\\) (i.e. target variable). In case of Linear Regression, the relation is just linear.\nSo, generally the Linear Regression hypothesis is represented as \u0026ndash;\n$$ h_{\\theta} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \u0026hellip; + \\theta_nx_n $$ $$ \\therefore h_{\\theta} = \\sum_{i=1}^{n} \\theta_ix_i $$\nAnd in vector form we can write the same as \u0026ndash;\n$$ y = h(x) = \\theta^Tx $$\nNote: \\(h_\\theta(x)\\) is same as \\(h(x)\\)\nIn any Machine Learning problem, we can have different kinds of errors. For example, we can have errors if we don\u0026rsquo;t consider some feature(s) to determine the result, although it is very relevant to the problem. It can also be a measurement error, or some random noise. So, the target variable can be written as \u0026ndash; $$ y^{(i)} = \\theta^Tx^{(i)} + \\epsilon^{(i)} $$\n\\(\\epsilon^{(i)}\\) accounts for the errors in our model. We assume that the error term of each training example, \\(\\epsilon^{(i)}\\) is Independently and Identically Distributed (IID). And they satisfy a Normal Distribution with mean 0, and standard deviation \\(\\sigma\\).\nAnd now you might ask why Normal(Gaussian) Distribution? Why not anything else?\nThe answer is given by Central Limit Theorem. Which tells us that if you take sufficiently large number of random numbers, then the sum of the numbers will be approximately normally distributed. So, this means that the error of one training example has the same distribution as the error of other training examples.\nAlso, \\(y^{(i)}\\) is a random variable that satisfies Normal distribution with the mean \\(\\theta^Tx\\) and variance \\(\\sigma^2\\). Now if we write the probability distribution of the predicted values \\(y\\), given \\(x\\) and the parameter \\(\\theta\\). It is given as follows:\n$$p(y^{(i)}|x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$\nThe expression on the left means that the distribution of \\(y^{(i)}\\) given \\(x^{(i)}\\) is parameterized by \\(\\theta\\).\nIn any general course, at this point we get introduced directly to the least squares, and we try to find the most likely \\(\\theta\\) for a given set of \\((y^i, x^i)\\) pairs by reducing the cost function. But, a more insightful way to understand this phenomena of reducing the cost function is to maximize the likelihood function.\nSo, what is a Likelihood function?\nIt is kind of a negative of loss function. But, fundamentally speaking it is the probability distribution function of \\(\\theta\\) where the data remains fixed.\nThe picture above taken from Statquest with Josh Stammer\u0026rsquo;s YouTube channel give the easiest idea of what Likelihood is about. You can check out his video for more details.\nSince we assume that every training example is IID, we can write for n training examples, the Likelihood function is \u0026ndash;\n$$L(\\theta) = \\prod_{i=1}^{n} p(y^{(i)}|x^{(i)}; \\theta)$$ $$\\implies L(\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$\nWe need to maximize this function in order to find the parameter values that give the distribution that maximizes the probability of observing the data. The parameter values (i.e. \\(\\theta\\)) are found such that they maximize the likelihood that the process described by the model produce the data that were actually observed.\nBut to find this function\u0026rsquo;s maximum is quite tough, so we can find the maximum of the \\(\\log\\) of the Likelihood function. This is only possible because the Likelihood function and the log of the Likelihood function both peak at the same point for the same \\(\\mu\\) and \\(\\sigma\\). Also, it\u0026rsquo;s way, way easier to find the maximum of the log Likelihood. So, let\u0026rsquo;s find it.\n$$l(\\theta) = log L(\\theta)$$ $$\\implies l(\\theta) = \\log \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$ $$\\implies l(\\theta) = \\sum_{i=1}^n \\log \\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$ $$\\implies l(\\theta) = n\\log \\frac{1}{\\sqrt{2\\pi}\\sigma} - \\frac{1}{\\sigma^2}\\cdot\\frac{1}{2}\\sum_{i=1}^{n}(y^{(i)}- \\theta^Tx^{(i)})^2$$\nAs, the first term is a constant anyway so to increase the function \\(l(\\theta)\\), we have to maximize the second term. The term which we may already know as the cost function. Written as \u0026ndash;\n$$J(\\theta) = \\frac {1}{2} \\sum_{i=1}^{n} (y^{(i)}- \\theta^Tx^{(i)})^2$$\nAnd hence proving that the cost function didn\u0026rsquo;t just drop from anywhere. And with this suddenly you don\u0026rsquo;t have to convince your intuition to agree that half of the sum of the squares of the distances of prediction from the actual value, is need to be reduced to get a better parameter, i.e. \\(\\theta\\). Now it feels like the cost function was always meant to be the cost function.\nWhat a beauty!\nI hope you found it beautiful as well. Please leave a comment and a reaction. Let me know, if I made any mistake. Thanks to Professor Andrew Ng for introducing me to such beauty! And thanks to you if you read this!\nReferences  CS229 lecture notes Statquest wit Josh Stammer\u0026rsquo;s YouTube channel Wikipedia articles about Likelihood function, IID  ","href":"/tTales/posts/lms/","title":"Finally Linear Regression is beautiful"},{"content":"","href":"/tTales/tags/machine-learning/","title":"Machine Learning"},{"content":"","href":"/tTales/posts/","title":"Posts"},{"content":"","href":"/tTales/tags/probability/","title":"Probability"},{"content":"","href":"/tTales/tags/regression/","title":"Regression"},{"content":"","href":"/tTales/tags/","title":"Tags"},{"content":"","href":"/tTales/","title":"tTales"},{"content":"","href":"/tTales/authors/","title":"Authors"},{"content":"","href":"/tTales/categories/","title":"Categories"},{"content":"","href":"/tTales/page/","title":"Pages"},{"content":"","href":"/tTales/search/","title":"Search"},{"content":"","href":"/tTales/series/","title":"Series"}]
