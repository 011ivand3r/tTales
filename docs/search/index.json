[{"content":"Hi, I\u0026rsquo;m Hans Ollivander (han5 011ivand3r). I am a student and dreamer. As a student, I ask questions and seek for digestible answers. Generally, it is followed by me trying to contradict my brain with wrong answers. And a dreamer, because that\u0026rsquo;s where freedom is.\nI am passionate about human space exploration, machine learning and storytelling. I often find myself dabbling in the reality of fickleness of carbon-based life on planet Earth. I consume books, articles, podcasts and documentaries on them. And talk to people about them (Not as much I want to! So why not get in touch!).\nI enjoy running and playing chess. Although, I\u0026rsquo;m quite a novice at both. But, I hope to participate in some events in future. And of course, I sleep a lot.\n Enough about me. tTales is all about dumping my thoughts and opinions into one place on the internet.\ntTales is created with HUGO. The theme I\u0026rsquo;m using is minimo. I customized it as per my needs, and it\u0026rsquo;s currently hosted on GitHub. The reason I chose minimo over other themes is the search bar and obviously the UI üòç.\nCheck out the HUGO site. They have hundreds of exciting themes, and it\u0026rsquo;s built on Go. So, it\u0026rsquo;s faster than other static site generators and very easy to set up. Now that I‚Äôve justified my choices, thanks for visiting my website!\nGet in touch using any of the links in the footer!\n","href":"/tTales/about/","title":"About"},{"content":"The first thing whenever one starts learning about Machine Learning algorithms is Linear Regression. But the way people generally blaze through it because it\u0026rsquo;s rudimentary and label it as very intuitive doesn\u0026rsquo;t sit well with me.\nSo, in this article we will build a Linear Regression model with the help of probability.\nRegression\u0026rsquo;s idea is to determine the strength of relation between a bunch of independent variables $x = (x_1, x_2, \u0026hellip; x_n)$ and a dependent variable $y$ (i.e. target variable).\nSo, generally the Linear Regression hypothesis is represented as \u0026ndash; $$ h_{\\theta} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \u0026hellip; + \\theta_nx_n $$ $$ \\therefore h_{\\theta} = \\sum_{i=1}^{n} \\theta_ix_i $$ And in vector form we can write the same as \u0026ndash; $$ y = h(x) = \\theta^Tx $$ Note: $h_\\theta(x)$ is same as $h(x)$\nIn a Machine Learning problem, we can have different kinds of errors. For example, we can have errors if we don\u0026rsquo;t consider some feature(s) to determine the result, although it is very relevant to the problem. It can also be a measurement error, or a random noise. So, the target variable can be written as \u0026ndash; $$ y^{(i)} = \\theta^Tx^{(i)} + \\epsilon^{(i)} $$\n$\\epsilon^{(i)}$ accounting for the errors in our model. We assume that the error term of each training example, $\\epsilon^{(i)}$ is Independently and Identically Distributed (IID). And they satisfy a Normal Distribution with mean 0, and standard deviation $\\sigma$.\nAnd now you might ask why Normal(Gaussian) Distribution? Why not anything else?\nThe answer is given by Central Limit Theorem. Which tells us that if you take sufficiently large number of random numbers, then the sum of the numbers will be approximately normally distributed. So, this means that the error of one training example has the same distribution as the error of other training examples.\nAlso, $y^{(i)}$ is a random variable that satisfies Normal distribution with the mean $\\theta^Tx$ and variance being $\\sigma^2$. Now if we write the probability distribution of the predicted values $y$, given $X$ and the parameter $\\theta$. It is given as follows: $$p(y^{(i)}|x^{(i)}; \\theta) = \\frac{1}{\\sqrt2\\pi\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$ This means that the distribution of $y^{(i)}$ given $x^{(i)}$ is parameterized by $\\theta$.\nNow here is where, we get introduced directly to the Ordinary Least Squares, and we try to find the most likely $\\theta$ for a given set of $(y^i, x^i)$ pairs by reducing the cost function. But, a more insightful way to find this estimate is to maximize the likelihood function.\nSo, what is Likelihood function?\nIt is kind of a negative of loss function. But, fundamentally speaking it is the probability distribution function of $\\theta$.\nThe picture above taken from Statquest wit Josh Stammer\u0026rsquo;s YouTube channel give the easiest idea of what Likelihood is about. You can check out his video for more details.\nSince we assume that every training example is IID, we can write for n training examples \u0026ndash;\n$$L(\\theta) = \\prod_{i=1}^{n} p(y^{(i)}|x^{(i)}; \\theta)$$ $$\\implies L(\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt2\\pi\\sigma}exp(-\\frac{(y^{(i)}- \\theta^Tx^{(i)})^2}{2\\sigma^2})$$\nWe need to maximize this function in order to find the parameter values that give the distribution that maximizes the probability of observing the data. The parameter values(i.e. $\\theta$) are found such that they maximize the likelihood that the process described by the model produce the data that were actually observed.\nTo find this function\u0026rsquo;s maximum is quite tough, so we can find the maximum of the $\\log$ of the Likelihood function. This is only possible because the Likelihood function and the log of the Likelihood function both peak at the same point for the same $\\mu$ and $\\sigma$. Also, it\u0026rsquo;s way, way easier to find the derivative.\nSo, let\u0026rsquo;s find the derivative.\n\\begin{align*} $$ L(\\mu|y)} \u0026amp;= afdsaf \\\n\u0026amp;= btcvgrw\\\n\u0026amp;= sfsfasdff$$ \\end{align*}\nAs, the first term is a constant anyway so to increase the function, is to maximize the second term. The term which we may already know as the cost function. Written as \u0026ndash; $$J(\\theta) = \\frac {1}{2} \\sum_{i=1}^{n} (y^{(i)}- \\theta^Tx^{(i)})^2$$\nNow, it feels like cost function was always meant to be the cost function. It didn\u0026rsquo;t just drop from anywhere. And suddenly you find yourself asking your intuition to agree that half of the sum of the squares of the distances of prediction from the actual value, is need to be reduced to get a better parameter, i.e. $\\theta$.\nIf you have come this far, please leave a comment and a reaction. Let me know, if I made any mistake. Thanks!\n","href":"/tTales/posts/lms/","title":"LMS"},{"content":"","href":"/tTales/tags/machine-learning/","title":"Machine Learning"},{"content":"","href":"/tTales/posts/","title":"Posts"},{"content":"","href":"/tTales/tags/probability/","title":"Probability"},{"content":"","href":"/tTales/tags/regression/","title":"Regression"},{"content":"","href":"/tTales/tags/","title":"Tags"},{"content":"","href":"/tTales/","title":"tTales"},{"content":"","href":"/tTales/authors/","title":"Authors"},{"content":"","href":"/tTales/categories/","title":"Categories"},{"content":"","href":"/tTales/page/","title":"Pages"},{"content":"","href":"/tTales/search/","title":"Search"},{"content":"","href":"/tTales/series/","title":"Series"}]
